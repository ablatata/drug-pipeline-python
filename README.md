## Table des matiÃ¨res
1. [Projet](#projet)
2. [Data Pipeline](#data-pipeline)
3. [Ad-hoc](#ad-hoc)
4. [Sql Requests](#sql-requests)

## Projet
***
Python est le langage de programmation utiliser pour dÃ©velopper le projet. le fichier requirements.txt liste les dÃ©pendances du projet.

## Data Pipeline
***
L'objectif de Ce pipeline est de produire en sortie un fichier JSON qui reprÃ©sente un graphe de liaison entre les diffÃ©rents mÃ©dicaments et leurs mentions respectives dans les diffÃ©rentes publications PubMed, les diffÃ©rentes publications scientifiques et enfin les journaux avec la date associÃ©e Ã  chacune de ces mentions. 
### ğŸ“š Inputs Data
| File | Count | Description |
|:--------------|:-------------:|--------------:|
| data/drugs.csv | 7 | un fichier csv contenant les donnÃ©es des drugs avec deux colonnes : atccode => id de drug, drug => nom de drug |  
|:--------------|:-------------:|--------------:|
| data/clinical_trials.csv | 8 | un fichier csv contenant les donnÃ©es des essais cliniques avec 4 colonnes : id => id de trial, scientific_title => le titre scientifique de trial, data: date de trial, journal: nom de journal de publication de trial |
|:--------------|:-------------:|--------------:|
| data/pubmed.csv | 7 | un fichier csv contenant les donnÃ©es des publications avec 4 colonnes : id => id de publication, title => le titre de la publication, data: date de la publication, journal: nom de journal de publication |
|:--------------|:-------------:|--------------:|
| data/pubmed.json | 5 | un fichier json contenantles donnÃ©es des publications avec 4 colonnes : id => id de publication, title => le titre de la publication, data: date de la publication, journal: nom de journal de publication |

### â­ Pipeline
| Workflow | Etapes |
|:--------------|:-------------:|
| All | Reading Inputs â†’ Data Cleaning â†’ Processing â†’ Loading Result |
***
   * 1. Reading Inputs : consiste Ã  lire la config et les donnÃ©es d'entrÃ©e en utilisant un connecteur; Dans notre cas, on a un seul type de connecteur :  un connecteur local.
   * 2. Data Cleaning : consiste Ã  : 
       * A. clean_date_column : permet de normaliser les dates afin d'avoir le mÃªme format : '%d-%m-%Y'
       * B. clean_words_column : permet de nettoyer les colonnes contenant des mots, d'enlever les caractÃ¨res non ascii, les espaces et de lower case ces strings.
       * C. deal_with_nan : permet de supprimer les nulls.
   * 3. Processing : permet de calculer les liens entre les differents medicaments et leurs mentions dans les PubMed, 
    les publications scientifiques et les journaux. L'output de cette Ã©tape prend la forme suivante : 
    ```
    [{"drug_id": drug_id_value, 
                     "links":[{
                                 "link_type": link_type_1, "journal_name" : journal_name_1, 
                                 "date_mention": date_mention_1},  ....],
                                
                    }]

    ```
    ***
    4. Loading Result: permet d'Ã©crire l'output de la pipeline dans un fichier json en utilisant le laoding. Dans notre cas, on a un seul type de loading : loading_local.

### â­ Run Pipeline  :
***
Cette pipeline peut Ãªtre lancÃ©e en utilsant : 
```
main.py 

```
## Ad-Hoc : 
***
Cette feature peut Ãªtre calculÃ©e en utilsant : 
```
main_features.py 

```
## Sql Requests :
*** 
*1.	1ere requÃªte : Calculer le montant total des ventes jour par jour : 
*sql/sales_total_amount_day_by_day
*2.	2eme requÃªte : Calculer le montant des ventes des meubles et deco par client.
*sql/meubles_deco_sales.sql

